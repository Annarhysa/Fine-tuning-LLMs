{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7858108,"sourceType":"datasetVersion","datasetId":4609245}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Describe your model -> fine-tuned LLaMA 2\nBy Matt Shumer (https://twitter.com/mattshumer_)\n\nThe goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n\nFirst, use the best GPU available (go to Runtime -> change runtime type)\n\nTo create your model, just go to the first code cell, and describe the model you want to build in the prompt. Be descriptive and clear.\n\nSelect a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n\nYou can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell.","metadata":{"_uuid":"c18c6ae9-c912-4a90-b666-d0e74227f0d8","_cell_guid":"95244ad9-cdb9-41f2-b293-e67bff7a1203","id":"wM8MRkf8Dr94","trusted":true}},{"cell_type":"markdown","source":"# Data generation step","metadata":{"_uuid":"8f81791e-e669-4531-b398-f2a58ea47b2c","_cell_guid":"a88f0803-d0d7-4eef-934f-1279bbf3069a","id":"Way3_PuPpIuE","trusted":true}},{"cell_type":"markdown","source":"Run this to generate the dataset.","metadata":{"_uuid":"509d590b-d6ab-47b3-8785-9deffb18a724","_cell_guid":"56e4c56a-b856-4ab6-a4c9-8fd27c0912c5","id":"1snNou5PrIci","trusted":true}},{"cell_type":"markdown","source":"Split into train and test sets.","metadata":{"_uuid":"cb035cc4-3ad8-43f6-8ac2-bd6be9d7e891","_cell_guid":"4382b59c-e238-486d-a4d1-6fe3dc1d4534","id":"A-8dt5qqtpgM","trusted":true}},{"cell_type":"markdown","source":"# Install necessary libraries","metadata":{"_uuid":"3aabf919-6740-4245-b430-fcdb93cfeb3f","_cell_guid":"de528bd3-6ef0-4f7d-bc7f-c321225904ce","id":"AbrFgrhG_xYi","trusted":true}},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"_uuid":"e20f8a3e-9daa-47e0-99aa-077140a692ba","_cell_guid":"31242b0c-1c45-4a67-bea1-c240f88aa8c9","collapsed":false,"id":"lPG7wEPetFx2","execution":{"iopub.status.busy":"2024-04-22T19:24:13.492846Z","iopub.execute_input":"2024-04-22T19:24:13.493222Z","iopub.status.idle":"2024-04-22T19:25:21.333029Z","shell.execute_reply.started":"2024-04-22T19:24:13.493191Z","shell.execute_reply":"2024-04-22T19:25:21.332125Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Hyperparameters","metadata":{"_uuid":"b87e9e98-bed3-42df-88f2-0e4ff5d0e7d6","_cell_guid":"f8991bb3-16bb-425d-b247-a904a79b799e","id":"moVo0led-6tu","trusted":true}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/coding-data/python_code_instructions.csv\")\ndf.drop('Unnamed: 0',axis = 1,inplace=True)\ntrain_df, eval_df = train_test_split(df, test_size = 0.15, random_state = 42)\n\ntrain_ds = Dataset.from_pandas(train_df, split=\"train\")\neval_ds = Dataset.from_pandas(eval_df, split=\"test\")\n\nfrom transformers import AutoTokenizer\nmodel_name_or_path = \"NousResearch/llama-2-7b-chat-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\ntrain_ds = train_ds.flatten()\ntype(train_ds)\ndef preprocess_function(examples):\n    if \"instruction\" in examples:  # Check if the column exists\n        return tokenizer([\" \".join(x) for x in examples[\"instruction\"]])\n    else:\n    # Handle the case where the column is missing (e.g., raise an error or use a default value)\n        raise ValueError(\"Instruction column not found in the dataset\")\n        return tokenizer([\" \".join(x) for x in train_ds[\"output\"]])\ntokenizer_train_ds = train_ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=train_ds.column_names,\n)\ntokenizer_eval_ds = eval_ds.map(\n    preprocess_function,\n    batched=True,\n    num_proc=4,\n    remove_columns=train_ds.column_names,\n)","metadata":{"_uuid":"eb5ef20d-527e-4e66-9d7c-06b8a8e26c37","_cell_guid":"dea930d6-3d40-4c7c-92a1-00c02dd6a917","collapsed":false,"execution":{"iopub.status.busy":"2024-04-22T20:05:21.908964Z","iopub.execute_input":"2024-04-22T20:05:21.909351Z","iopub.status.idle":"2024-04-22T20:05:28.887063Z","shell.execute_reply.started":"2024-04-22T20:05:21.909319Z","shell.execute_reply":"2024-04-22T20:05:28.885665Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_data","metadata":{"_uuid":"7b604694-04bc-45af-8e59-31e52b479627","_cell_guid":"8d279e8c-91fc-4fa1-b650-689fcb4bb286","collapsed":false,"execution":{"iopub.status.busy":"2024-04-22T19:52:43.114820Z","iopub.execute_input":"2024-04-22T19:52:43.115580Z","iopub.status.idle":"2024-04-22T19:52:43.121969Z","shell.execute_reply.started":"2024-04-22T19:52:43.115543Z","shell.execute_reply":"2024-04-22T19:52:43.120824Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_size = 128\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\ncode_data =  tokenizer_train_ds.map(group_texts, batched=True, num_proc=4)\neval_data = tokenizer_eval_ds.map(group_texts, batched=True, num_proc=4)","metadata":{"_uuid":"bd79c9c7-4ab6-4e60-8dfc-5ee9d6eb7067","_cell_guid":"a3a89379-6736-4f0e-bdb4-34b0c5307275","collapsed":false,"execution":{"iopub.status.busy":"2024-04-22T20:05:59.244635Z","iopub.execute_input":"2024-04-22T20:05:59.245024Z","iopub.status.idle":"2024-04-22T20:06:06.168809Z","shell.execute_reply.started":"2024-04-22T20:05:59.244988Z","shell.execute_reply":"2024-04-22T20:06:06.167818Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"NousResearch/llama-2-7b-chat-hf\" # use this if you have access to the official LLaMA 2 model \"meta-llama/Llama-2-7b-chat-hf\", though keep in mind you'll need to pass a Hugging Face key argument\ndataset_name = \"/content/train.jsonl\"\nnew_model = \"llama-2-7b-custom\"\nlora_r = 64\nlora_alpha = 16\nlora_dropout = 0.1\nuse_4bit = True\nbnb_4bit_compute_dtype = \"float16\"\nbnb_4bit_quant_type = \"nf4\"\nuse_nested_quant = False\noutput_dir = \"./results\"\nnum_train_epochs = 1\nfp16 = False\nbf16 = False\nper_device_train_batch_size = 4\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 1\ngradient_checkpointing = True\nmax_grad_norm = 0.3\nlearning_rate = 2e-4\nweight_decay = 0.001\noptim = \"paged_adamw_32bit\"\nlr_scheduler_type = \"constant\"\nmax_steps = -1\nwarmup_ratio = 0.03\ngroup_by_length = True\nsave_steps = 25\nlogging_steps = 5\nmax_seq_length = None\npacking = False\ndevice_map = {\"\": 0}","metadata":{"_uuid":"c7ef2c40-4d56-45f3-aa1f-4cab9120644d","_cell_guid":"fc8c8847-d438-4f25-a66e-69d4c58d64e4","collapsed":false,"id":"bqfbhUZI-4c_","execution":{"iopub.status.busy":"2024-04-22T20:06:08.534015Z","iopub.execute_input":"2024-04-22T20:06:08.534976Z","iopub.status.idle":"2024-04-22T20:06:08.542644Z","shell.execute_reply.started":"2024-04-22T20:06:08.534934Z","shell.execute_reply":"2024-04-22T20:06:08.541762Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Load Datasets and Train","metadata":{"_uuid":"91873473-36f8-4025-8c15-61f87d3aa8fc","_cell_guid":"e3a629cc-88ab-48f9-b0c3-fb6ab6b08074","id":"F-J5p5KS_MZY","trusted":true}},{"cell_type":"code","source":"train_df.columns","metadata":{"_uuid":"6c19e0f6-1c23-4b77-8b84-1fb11ffe0096","_cell_guid":"1b41dff6-fc3c-4ea5-835c-f6ab23650e49","collapsed":false,"execution":{"iopub.status.busy":"2024-04-22T20:06:10.804496Z","iopub.execute_input":"2024-04-22T20:06:10.804885Z","iopub.status.idle":"2024-04-22T20:06:10.811778Z","shell.execute_reply.started":"2024-04-22T20:06:10.804855Z","shell.execute_reply":"2024-04-22T20:06:10.810778Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\n# train_dataset = load_dataset('json', data_files='/content/train.jsonl', split=\"train\")\n# valid_dataset = load_dataset('json', data_files='/content/test.jsonl', split=\"train\")\n\n# Preprocess datasets\n# train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n# valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n\n# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n# bnb_config = BitsAndBytesConfig(\n#     load_in_4bit=use_4bit,\n#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n#     bnb_4bit_compute_dtype=compute_dtype,\n#     bnb_4bit_use_double_quant=use_nested_quant,\n# )\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_name,\n#     quantization_config=bnb_config,\n#     device_map=device_map\n# )\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"all\",\n    evaluation_strategy=\"steps\",\n    eval_steps=5  # Evaluate every 20 steps\n)\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_ds,\n    eval_dataset=eval_ds,  # Pass validation dataset here\n    peft_config=peft_config,\n    dataset_text_field=\"instruction\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\ntrainer.train()\ntrainer.model.save_pretrained(new_model)\n\n# Cell 4: Test the model\nlogging.set_verbosity(logging.CRITICAL)\n# prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\n# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n# result = pipe(prompt)\n# print(result[0]['generated_text'])","metadata":{"_uuid":"05da2887-c38c-47ef-9816-e22a80bb2cd1","_cell_guid":"84236bee-7de3-43c3-a2e9-557238f7f944","collapsed":false,"id":"qf1qxbiF-x6p","execution":{"iopub.status.busy":"2024-04-22T20:08:14.849312Z","iopub.execute_input":"2024-04-22T20:08:14.849982Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run Inference","metadata":{"_uuid":"9691149d-b1ba-4446-8e98-84af5773069b","_cell_guid":"2e01d48f-d2d5-436c-acfe-c05d0c38503d","id":"F6fux9om_c4-","trusted":true}},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function that reverses a string. [/INST]\" # replace the command here with something relevant to your task\nnum_new_tokens = 100  # change to the number of new tokens you want to generate\n\n# Count the number of tokens in the prompt\nnum_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n\n# Calculate the maximum length for the generation\nmax_length = num_prompt_tokens + num_new_tokens\n\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\nresult = gen(prompt)\nprint(result[0]['generated_text'].replace(prompt, ''))","metadata":{"_uuid":"2c7c8b60-042c-46fb-bac6-dd72f306430a","_cell_guid":"5410ad31-aa1e-4c60-9635-9d6de0bf18b1","collapsed":false,"id":"7hxQ_Ero2IJe","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Merge the model and store in Google Drive","metadata":{"_uuid":"b2f3f946-02ed-4483-be98-9e6f407bc899","_cell_guid":"f02a0b5f-8b00-4e39-bea9-bc227c2ab55b","id":"Ko6UkINu_qSx","trusted":true}},{"cell_type":"code","source":"# Merge and save the fine-tuned model\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to your preferred path\n\n# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Save the merged model\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"_uuid":"ccd9990f-08a1-4804-ad09-172f01a56b26","_cell_guid":"00c78190-93ea-40fd-b41f-6f5f13996728","collapsed":false,"id":"AgKCL7fTyp9u","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load a fine-tuned model from Drive and run inference","metadata":{"_uuid":"8e046479-61e8-40e7-b926-8bf106a049b7","_cell_guid":"26fa09d3-0dfc-42bf-b579-a978417faaab","id":"do-dFdE5zWGO","trusted":true}},{"cell_type":"code","source":"from google.colab import drive\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndrive.mount('/content/drive')\n\nmodel_path = \"/content/drive/MyDrive/llama-2-7b-custom\"  # change to the path where your model is saved\n\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"_uuid":"5b74df2a-d646-4480-b77e-78e1f491e56e","_cell_guid":"37729b30-1243-432f-9090-f45240b533f3","collapsed":false,"id":"xg6nHPsLzMw-","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nprompt = \"What is 2 + 2?\"  # change to your desired prompt\ngen = pipeline('text-generation', model=model, tokenizer=tokenizer)\nresult = gen(prompt)\nprint(result[0]['generated_text'])","metadata":{"_uuid":"5454358e-1e1d-4f77-bf5a-7456693f7da7","_cell_guid":"0ff7c48a-88f4-4c58-9337-baaae667fdb0","collapsed":false,"id":"fBK2aE2KzZ05","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}